{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "01. What is a parameter?"
      ],
      "metadata": {
        "id": "63r8pEue2-Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS:- A parameter is a placeholder in a function definition that specifies what kind of data the function expects to receive."
      ],
      "metadata": {
        "id": "cofBmIYU3d9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "02. What is correlation?"
      ],
      "metadata": {
        "id": "WC40anWd3rPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- **correlation** usually refers to **calculating how strongly two sets of data are related**—just like in statistics—but it's done using code.\n",
        "\n"
      ],
      "metadata": {
        "id": "4SJkMzpc3v4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does negative correlation mean?"
      ],
      "metadata": {
        "id": "VQ09-8mX4JBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS;- a negative correlation means that as one variable increases, the other tends to decrease—and this relationship is discovered and measured through code, often using data analysis libraries."
      ],
      "metadata": {
        "id": "WIicQpEX4XsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "03. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "9Oy2cQ464Nxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn from data and make decisions or predictions without being explicitly programmed for every specific task.\n",
        "\n",
        "Instead of following hard-coded rules, ML models find patterns in data and use those patterns to make predictions or decisions."
      ],
      "metadata": {
        "id": "pXnNqWjp4jTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "04. How does loss value help in determining whether the model is good or not?\n"
      ],
      "metadata": {
        "id": "x8XzMJA14rpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- The loss value (also called cost) is a key indicator of how good or bad your machine learning model is at making predictions. It tells you how far off your model's predictions are from the actual values.\n",
        "\n"
      ],
      "metadata": {
        "id": "-qC92j1q41en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "05. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "7hS7HZuE4-oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- ### 📊 **Continuous vs. Categorical Variables**\n",
        "\n",
        "In data analysis and machine learning, understanding the **type of variable** you're working with is crucial. Two main types are:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 🔢 **Continuous Variables**\n",
        "\n",
        "These are **numeric variables** that can take on **any value within a range**. They are typically measured and have infinite possible values (in theory).\n",
        "\n",
        "#### ✅ Characteristics:\n",
        "\n",
        "* Measurable quantities\n",
        "* Can have decimals\n",
        "* Values can be ordered and compared\n",
        "\n",
        "#### 📌 Examples:\n",
        "\n",
        "* Height (e.g., 170.5 cm)\n",
        "* Weight (e.g., 65.2 kg)\n",
        "* Temperature (e.g., 98.6°F)\n",
        "* Income (e.g., \\$45,500.75)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 🔠 **Categorical Variables**\n",
        "\n",
        "These represent **categories or groups**. They are **not numerical** (or treated as labels even if numbers are used).\n",
        "\n",
        "#### ✅ Characteristics:\n",
        "\n",
        "* Represent types or categories\n",
        "* Often limited to a fixed set of values\n",
        "* Can be nominal or ordinal\n",
        "\n",
        "#### 🧩 Types:\n",
        "\n",
        "* **Nominal**: No natural order\n",
        "  e.g., `[\"Red\", \"Blue\", \"Green\"]`, `[\"Dog\", \"Cat\", \"Bird\"]`\n",
        "\n",
        "* **Ordinal**: Ordered categories\n",
        "  e.g., `[\"Low\", \"Medium\", \"High\"]`, `[\"Beginner\", \"Intermediate\", \"Expert\"]`\n",
        "\n",
        "#### 📌 Examples:\n",
        "\n",
        "* Gender (`Male`, `Female`)\n",
        "* City (`New York`, `London`, `Tokyo`)\n",
        "* Yes/No (`0`, `1`)\n",
        "* Education level (`High School`, `Bachelor's`, `Master's`)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It Matters in Machine Learning:\n",
        "\n",
        "* Continuous variables are typically used **as-is** in models.\n",
        "* Categorical variables often need to be **encoded** (e.g., one-hot encoding or label encoding) before being used in ML models.\n",
        "\n"
      ],
      "metadata": {
        "id": "yK8tFbFl5CaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "06. How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?"
      ],
      "metadata": {
        "id": "cOpdWpy_5RLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-  Handling **categorical variables** correctly is essential in machine learning, because most ML algorithms can't directly process non-numeric data. You need to **convert categories into numbers** while preserving meaning (where relevant).\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ **Common Techniques to Handle Categorical Variables**\n",
        "\n",
        "#### 1. **Label Encoding**\n",
        "\n",
        "* **What it does**: Assigns a unique number to each category.\n",
        "* **Best for**: Ordinal variables (where the order matters).\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'Medium', 'High']\n",
        "encoder = LabelEncoder()\n",
        "encoded = encoder.fit_transform(data)\n",
        "print(encoded)  # Output: [1, 2, 0] (for example)\n",
        "```\n",
        "\n",
        "> ⚠️ Not ideal for **nominal** data (e.g., `['Red', 'Blue', 'Green']`) because ML models may misinterpret the numbers as having order.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **One-Hot Encoding**\n",
        "\n",
        "* **What it does**: Creates binary columns (0 or 1) for each category.\n",
        "* **Best for**: Nominal variables (where order does **not** matter).\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue']})\n",
        "encoded = pd.get_dummies(df)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "   Color_Blue  Color_Green  Color_Red\n",
        "0           0            0          1\n",
        "1           0            1          0\n",
        "2           1            0          0\n",
        "```\n",
        "\n",
        "> ✅ Widely used. Most models (like logistic regression, tree-based models) work well with this.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Ordinal Encoding**\n",
        "\n",
        "* **What it does**: Assigns ordered integers to ordered categories (like label encoding).\n",
        "* **Best for**: When the categories have a clear order (e.g., `['Low', 'Medium', 'High']`).\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Low'], ['Medium'], ['High']]\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "encoded = encoder.fit_transform(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Frequency Encoding**\n",
        "\n",
        "* **What it does**: Replaces each category with its frequency count in the dataset.\n",
        "* **Best for**: High-cardinality variables (many unique values).\n",
        "\n",
        "```python\n",
        "df['Category_freq'] = df['Category'].map(df['Category'].value_counts())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Target Encoding (Mean Encoding)**\n",
        "\n",
        "* **What it does**: Replaces a category with the average target value for that category.\n",
        "* **Best for**: Categorical features with many levels in supervised learning.\n",
        "\n",
        "> ⚠️ Risk of **data leakage** if not done properly (use cross-validation).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Summary Table:\n",
        "\n",
        "| Technique          | Best For            | Handles Order?            | Creates New Columns? |\n",
        "| ------------------ | ------------------- | ------------------------- | -------------------- |\n",
        "| Label Encoding     | Ordinal             | ✅ Yes                     | ❌ No                 |\n",
        "| One-Hot Encoding   | Nominal             | ❌ No                      | ✅ Yes                |\n",
        "| Ordinal Encoding   | Ordinal             | ✅ Yes                     | ❌ No                 |\n",
        "| Frequency Encoding | High-cardinality    | ❌ No                      | ❌ No                 |\n",
        "| Target Encoding    | Supervised problems | ❌ No (unless designed to) | ❌ No                 |\n",
        "\n",
        "---\n",
        "\n",
        "### 🚧 When Choosing a Technique:\n",
        "\n",
        "* Use **one-hot encoding** for low-cardinality nominal features.\n",
        "* Use **label or ordinal encoding** for ordered categories.\n",
        "* Use **target/frequency encoding** for high-cardinality features (e.g., countries, product IDs).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dswwjO0A5VkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "07. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "6MivB8BD5nYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- ### 🧠 **Training vs. Testing\n",
        "\n",
        "| Term             | Meaning                                        | Purpose                       |\n",
        "| ---------------- | ---------------------------------------------- | ----------------------------- |\n",
        "| **Training Set** | Data used to **teach** the model               | Learn patterns                |\n",
        "| **Test Set**     | Data used to **evaluate** the model's accuracy | Check performance on new data |\n",
        "\n",
        "* **Training** = Model **learns**\n",
        "* **Testing** = Model is **tested** on unseen data\n",
        "\n",
        "> Goal: Good performance on both = model is **generalizing** well.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJNy7Ni559X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "08. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "6I5sw-pG6L1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- **`sklearn.preprocessing`** is a module in **scikit-learn** (a popular Python machine learning library) that provides tools to **prepare and transform your data** before feeding it into machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 What Does It Do?\n",
        "\n",
        "* **Scale features** (e.g., normalize or standardize)\n",
        "* **Encode categorical variables** (e.g., label encoding, one-hot encoding)\n",
        "* **Generate polynomial features**\n",
        "* **Impute missing values** (in some related modules)\n",
        "* **Other transformations** (like binarizing data)\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ Common Classes and Functions in `sklearn.preprocessing`:\n",
        "\n",
        "| Tool                 | Purpose                                         |\n",
        "| -------------------- | ----------------------------------------------- |\n",
        "| `StandardScaler`     | Scale features to have mean=0, std=1            |\n",
        "| `MinMaxScaler`       | Scale features to a given range (0 to 1)        |\n",
        "| `LabelEncoder`       | Convert labels to numeric values                |\n",
        "| `OneHotEncoder`      | Convert categorical variables to binary vectors |\n",
        "| `PolynomialFeatures` | Create polynomial and interaction features      |\n",
        "| `Binarizer`          | Convert numerical data to binary (0 or 1)       |\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Standardizing Features\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your feature matrix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use It?\n",
        "\n",
        "* Many ML algorithms perform better when data is **scaled** or **properly encoded**.\n",
        "* Preprocessing improves model **convergence** and **accuracy**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I0IHDkpb6VJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "09. What is a Test set?"
      ],
      "metadata": {
        "id": "as07O18m6bzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- ### 🧪 **Test Set**\n",
        "\n",
        "A **test set** is a portion of your dataset that you **set aside and don’t use during training**. Its main purpose is to **evaluate how well your trained machine learning model performs on new, unseen data**.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "* Used **after training** to measure model accuracy.\n",
        "* Helps check if the model **generalizes well** or just memorized training data.\n",
        "* Usually represents **20-30%** of the total dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Simple analogy:\n",
        "\n",
        "If training data is the “practice exam,” the test set is the **“final exam”** that evaluates real understanding.\n",
        "\n"
      ],
      "metadata": {
        "id": "qtBAmosv6g_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "U9KggKuk6mK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- Here’s how you **split data into training and testing sets in Python** using **scikit-learn**:\n",
        "\n",
        "---\n",
        "\n",
        "### Using `train_test_split` from `sklearn.model_selection`\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose you have features X and target labels y\n",
        "X = [...]  # Your feature data (e.g., list, numpy array, pandas DataFrame)\n",
        "y = [...]  # Your labels/targets\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Now:\n",
        "# X_train, y_train -> data used to train the model\n",
        "# X_test, y_test -> data used to test/evaluate the model\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Parameters:\n",
        "\n",
        "* `test_size=0.2`: 20% of data goes to the test set.\n",
        "* `random_state=42`: Ensures the split is reproducible (same every run).\n",
        "\n",
        "---\n",
        "\n",
        "### Quick summary:\n",
        "\n",
        "* Use **`train_test_split`** to divide your dataset.\n",
        "* Train on the **training set**.\n",
        "* Evaluate on the **test set**.\n",
        "\n"
      ],
      "metadata": {
        "id": "SiiT191R6qPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "jTrzpMME6vTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "\n",
        "\n",
        "### 🧩 **Step-by-Step Approach to a Machine Learning Problem**\n",
        "\n",
        "1. **Understand the Problem**\n",
        "\n",
        "   * What’s the goal? (e.g., classification, regression)\n",
        "   * What kind of data do you have?\n",
        "\n",
        "2. **Collect Data**\n",
        "\n",
        "   * Gather all relevant data from sources (databases, files, APIs).\n",
        "\n",
        "3. **Explore and Analyze Data**\n",
        "\n",
        "   * Understand data types, distributions, missing values.\n",
        "   * Visualize relationships and detect anomalies.\n",
        "\n",
        "4. **Prepare and Clean Data**\n",
        "\n",
        "   * Handle missing values, remove duplicates, fix errors.\n",
        "   * Encode categorical variables, normalize/scale features.\n",
        "\n",
        "5. **Split Data**\n",
        "\n",
        "   * Divide data into training, validation, and testing sets.\n",
        "\n",
        "6. **Select and Train a Model**\n",
        "\n",
        "   * Choose suitable ML algorithms (e.g., decision trees, neural networks).\n",
        "   * Train models on the training set.\n",
        "\n",
        "7. **Evaluate Model Performance**\n",
        "\n",
        "   * Use metrics like accuracy, precision, recall, RMSE on validation/test sets.\n",
        "   * Compare different models and tune hyperparameters.\n",
        "\n",
        "8. **Improve the Model**\n",
        "\n",
        "   * Feature engineering (create new features).\n",
        "   * Try different algorithms, tune hyperparameters.\n",
        "\n",
        "9. **Deploy the Model**\n",
        "\n",
        "   * Put the model into production for real-world use.\n",
        "\n",
        "10. **Monitor and Maintain**\n",
        "\n",
        "    * Track model performance over time.\n",
        "    * Update model with new data if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 Summary\n",
        "\n",
        "| Step                     | Purpose                           |\n",
        "| ------------------------ | --------------------------------- |\n",
        "| Understand the problem   | Define goal and data needs        |\n",
        "| Collect data             | Get raw data                      |\n",
        "| Explore & clean data     | Prepare data for training         |\n",
        "| Split data               | Separate for unbiased evaluation  |\n",
        "| Train model              | Learn from data                   |\n",
        "| Evaluate & improve model | Measure and enhance accuracy      |\n",
        "| Deploy & monitor         | Use model in real life and update |\n"
      ],
      "metadata": {
        "id": "sLoORIIW6z3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "65_1eHPe69nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- Here's why **Exploratory Data Analysis (EDA)** is important before fitting a machine learning model:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 **Why Perform EDA Before Modeling?**\n",
        "\n",
        "1. **Understand Your Data**\n",
        "\n",
        "   * Discover data types, distributions, and ranges.\n",
        "   * Identify patterns and relationships between variables.\n",
        "\n",
        "2. **Detect Data Quality Issues**\n",
        "\n",
        "   * Find missing values, outliers, or errors that could hurt model performance.\n",
        "   * Decide how to handle those issues (e.g., imputation, removal).\n",
        "\n",
        "3. **Feature Selection & Engineering**\n",
        "\n",
        "   * Identify important features or create new ones to improve model accuracy.\n",
        "   * Drop irrelevant or redundant variables.\n",
        "\n",
        "4. **Choose the Right Model & Techniques**\n",
        "\n",
        "   * Knowing data characteristics helps pick suitable algorithms (e.g., linear vs. tree-based).\n",
        "   * Decide if scaling or encoding is needed.\n",
        "\n",
        "5. **Prevent Garbage-In Garbage-Out (GIGO)**\n",
        "\n",
        "   * Bad data leads to bad models. EDA helps ensure data quality, so the model learns meaningful patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "7U4_f6Rj7EkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?"
      ],
      "metadata": {
        "id": "srMiSYfA7QL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- **Correlation** is a statistical measure that describes the **strength and direction of the relationship** between two variables.\n",
        "\n",
        "---\n",
        "\n",
        "### Key points:\n",
        "\n",
        "* It shows how one variable **changes when the other changes**.\n",
        "* The correlation coefficient ranges from **-1 to +1**:\n",
        "\n",
        "  * **+1** means perfect positive correlation (both increase together).\n",
        "  * **0** means no correlation (no relationship).\n",
        "  * **-1** means perfect negative correlation (one increases, the other decreases).\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "* Height and weight usually have a **positive correlation** (taller people tend to weigh more).\n",
        "* Time spent watching TV and exam scores might have a **negative correlation** (more TV, lower scores).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKwoFzjq7ZE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "BeUqiAwT7hdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- Negative correlation = as one variable goes up, the other goes down."
      ],
      "metadata": {
        "id": "OJkaqlB17lac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "ET58NyKa7v-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- You can find correlation between variables in Python easily using **pandas** or **NumPy**. Here’s how:\n",
        "\n",
        "---\n",
        "\n",
        "### Using **pandas**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'X': [1, 2, 3, 4, 5],\n",
        "    'Y': [5, 4, 3, 2, 1],\n",
        "    'Z': [2, 3, 4, 5, 6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "print(corr_matrix)\n",
        "```\n",
        "\n",
        "This will output the correlation coefficients between all pairs of variables:\n",
        "\n",
        "```\n",
        "     X    Y    Z\n",
        "X  1.0 -1.0  1.0\n",
        "Y -1.0  1.0 -1.0\n",
        "Z  1.0 -1.0  1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Using **NumPy** for correlation between two variables:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "corr = np.corrcoef(x, y)[0, 1]\n",
        "print(corr)  # Output: -1.0 (perfect negative correlation)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "T_bY_fea7y-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "5Z_OV51876SU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans;-\n",
        "\n",
        "**Causation** means that one event **directly causes** another to happen. In other words, a change in variable A **produces** a change in variable B.\n",
        "\n",
        "---\n",
        "\n",
        "### Difference Between **Correlation** and **Causation**\n",
        "\n",
        "| Aspect              | Correlation                                                      | Causation                                   |\n",
        "| ------------------- | ---------------------------------------------------------------- | ------------------------------------------- |\n",
        "| Meaning             | Variables **move together** (linked)                             | One variable **directly affects** the other |\n",
        "| Direction           | No cause-effect implied                                          | Clear cause and effect relationship         |\n",
        "| Example             | Ice cream sales ↑ and drowning incidents ↑                       | Smoking **causes** lung cancer              |\n",
        "| Can it be spurious? | Yes, can be coincidence or caused by a third factor (confounder) | No, causal relationship is direct           |\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "* **Correlation**:\n",
        "  Ice cream sales and drowning rates both increase during summer.\n",
        "  They are **correlated** because both go up together, but ice cream sales **don’t cause** drowning.\n",
        "\n",
        "* **Causation**:\n",
        "  Smoking **causes** lung cancer. This is a direct cause-effect relationship backed by evidence.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t9cHhoWW7-eU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "_tILtEvQ8HRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- ### What is an **Optimizer** in Machine Learning?\n",
        "\n",
        "An **optimizer** is an algorithm or method used to **adjust the parameters** (like weights in neural networks) of a model during training to **minimize the loss function** (error).\n",
        "\n",
        "* It helps the model **learn** by finding the best parameters that reduce prediction errors.\n",
        "* The process is called **optimization**.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Types of Optimizers & Their Explanation\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Gradient Descent (GD)**\n",
        "\n",
        "* The most basic optimizer.\n",
        "* It updates parameters by moving **opposite to the gradient** of the loss function.\n",
        "* Uses the **whole training dataset** to compute gradients each step.\n",
        "\n",
        "**Example:**\n",
        "For a function $f(w)$, update rule is:\n",
        "$w = w - \\eta \\cdot \\nabla f(w)$\n",
        "where $\\eta$ = learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "* Similar to GD, but updates parameters using **one training example at a time**.\n",
        "* Faster per update but noisier steps.\n",
        "* Good for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Mini-batch Gradient Descent**\n",
        "\n",
        "* A compromise between GD and SGD.\n",
        "* Updates parameters using a **small batch** of data at each step.\n",
        "* Balances speed and stability.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Momentum**\n",
        "\n",
        "* Helps accelerate SGD by adding a fraction of the previous update to the current update.\n",
        "* Smooths updates and speeds up convergence, especially in ravines.\n",
        "\n",
        "**Update rule:**\n",
        "$v = \\gamma v + \\eta \\nabla f(w)$\n",
        "$w = w - v$\n",
        "where $\\gamma$ is momentum term.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **AdaGrad**\n",
        "\n",
        "* Adapts learning rate for each parameter individually.\n",
        "* Larger updates for infrequent parameters, smaller for frequent ones.\n",
        "* Good for sparse data.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **RMSProp**\n",
        "\n",
        "* Improves AdaGrad by using a moving average of squared gradients.\n",
        "* Prevents learning rate from shrinking too much.\n",
        "* Popular in training deep networks.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "* Combines ideas of Momentum and RMSProp.\n",
        "* Maintains moving averages of gradients and squared gradients.\n",
        "* One of the most widely used optimizers today.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Optimizer        | Description                                          | Use Case                              |\n",
        "| ---------------- | ---------------------------------------------------- | ------------------------------------- |\n",
        "| Gradient Descent | Full dataset gradient, slow but stable               | Small datasets                        |\n",
        "| Stochastic GD    | One sample per update, noisy but fast                | Large datasets                        |\n",
        "| Mini-batch GD    | Small batches per update, balance of speed/stability | Most deep learning models             |\n",
        "| Momentum         | Adds velocity to updates, accelerates convergence    | Deep networks with complex landscapes |\n",
        "| AdaGrad          | Adaptive learning rates per parameter                | Sparse data                           |\n",
        "| RMSProp          | Adaptive learning rates with moving average          | Recurrent Neural Networks, Deep nets  |\n",
        "| Adam             | Combines momentum and RMSProp, adaptive              | Most popular, works well generally    |\n",
        "\n",
        "---\n",
        "\n",
        "### Example (Using Adam in TensorFlow)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "model = ...  # define your model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "model.fit(X_train, y_train, epochs=10)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VctrJEHY8PzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "kZJ1vYaj8WmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- **`sklearn.linear_model`** is a module in **scikit-learn** that contains implementations of various **linear models** for regression and classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### What Does It Provide?\n",
        "\n",
        "* Algorithms that model the relationship between a dependent variable and one or more independent variables **assuming a linear relationship**.\n",
        "* Common models include:\n",
        "\n",
        "  * **Linear Regression** (for continuous output)\n",
        "  * **Logistic Regression** (for binary/multi-class classification)\n",
        "  * **Ridge Regression**, **Lasso Regression** (regularized linear models)\n",
        "  * **ElasticNet** (combines Ridge and Lasso)\n",
        "  * **Perceptron**, **SGDClassifier**, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use `sklearn.linear_model`?\n",
        "\n",
        "* Simple, interpretable models.\n",
        "* Good baseline models for regression/classification.\n",
        "* Often fast and efficient for large datasets.\n",
        "* Supports regularization to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Linear Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)  # Train the model\n",
        "predictions = model.predict(X_test)  # Predict on test data\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Model                | Task           | Description                       |\n",
        "| -------------------- | -------------- | --------------------------------- |\n",
        "| `LinearRegression`   | Regression     | Predict continuous values         |\n",
        "| `LogisticRegression` | Classification | Predict categorical classes       |\n",
        "| `Ridge`, `Lasso`     | Regression     | Linear models with regularization |\n",
        "| `SGDClassifier`      | Classification | Linear classifier with SGD        |\n",
        "\n"
      ],
      "metadata": {
        "id": "HMr1szBr8bhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "DG8tvTpn8guz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ansa;-\n",
        "\n",
        "The `fit()` method **trains the machine learning model** using your data. It means the model **learns the relationship** between the input features and the target labels by adjusting its internal parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### What arguments does it take?\n",
        "\n",
        "The most common arguments are:\n",
        "\n",
        "* `X` — **Feature matrix:** The input data (usually a 2D array or DataFrame), where each row is a sample and each column is a feature.\n",
        "* `y` — **Target vector:** The labels or values corresponding to each sample in `X`.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `X_train` is your training data (features).\n",
        "* `y_train` is the target/output for those samples.\n",
        "\n",
        "---\n",
        "\n",
        "### Additional optional arguments (depending on the model):\n",
        "\n",
        "* `sample_weight` — To give different weights to training samples.\n",
        "* `epochs`, `batch_size` — In deep learning libraries.\n",
        "* Others specific to some algorithms.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w8G5oVRM8mgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "bVnBQgB68uFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "The `predict()` method **uses the trained model to make predictions** on new input data.\n",
        "\n",
        "* After training (`fit()`), `predict()` applies the learned patterns to unseen data.\n",
        "* It outputs the **predicted labels or values** depending on the task (classification or regression).\n",
        "\n",
        "---\n",
        "\n",
        "### What arguments does it take?\n",
        "\n",
        "* `X` — The input features (new data you want predictions for). Usually a 2D array or DataFrame with the same number of features as the training data.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "```python\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `X_test` is new/unseen data.\n",
        "* `predictions` will contain the model’s output (e.g., predicted class labels or continuous values).\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Method      | Purpose                       | Required Argument             |\n",
        "| ----------- | ----------------------------- | ----------------------------- |\n",
        "| `predict()` | Generate predictions on input | `X` (features for prediction) |\n",
        "\n"
      ],
      "metadata": {
        "id": "nBiHOF4184SK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "feVQjsOO9Byz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans;-\n",
        "\n",
        "### **Continuous Variables**\n",
        "\n",
        "* Variables that can take **any numeric value** within a range.\n",
        "* Usually measured quantities.\n",
        "* Examples:\n",
        "\n",
        "  * Height (e.g., 170.5 cm)\n",
        "  * Temperature (e.g., 22.3°C)\n",
        "  * Age (if measured precisely)\n",
        "\n",
        "---\n",
        "\n",
        "### **Categorical Variables**\n",
        "\n",
        "* Variables that represent **categories or groups**.\n",
        "* Usually have a **fixed set of possible values** (labels).\n",
        "* Examples:\n",
        "\n",
        "  * Gender (Male, Female)\n",
        "  * Color (Red, Blue, Green)\n",
        "  * Type of vehicle (Car, Bike, Truck)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yks1MWLb9JJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "S1SQP4F39TYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans;-\n",
        "\n",
        "**Feature scaling** is the process of **normalizing or standardizing** the range of independent variables (features) in your data.\n",
        "\n",
        "---\n",
        "\n",
        "### Why do we need it?\n",
        "\n",
        "* Different features may have **different units and scales** (e.g., age in years vs. income in thousands).\n",
        "* Many ML algorithms **work better or converge faster** when features are on a similar scale.\n",
        "* Prevents features with larger scales from **dominating** the learning process.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Methods of Feature Scaling:\n",
        "\n",
        "| Method                                | What it does                                      | Result example                     |\n",
        "| ------------------------------------- | ------------------------------------------------- | ---------------------------------- |\n",
        "| **Normalization** (Min-Max Scaling)   | Scales features to a fixed range, usually \\[0, 1] | Age from 18–90 scaled to 0–1       |\n",
        "| **Standardization** (Z-score scaling) | Centers features to mean=0, std=1                 | Feature values like -1.2, 0.3, 1.5 |\n",
        "\n",
        "---\n",
        "\n",
        "### How it helps in ML:\n",
        "\n",
        "* **Gradient-based algorithms** (like logistic regression, neural networks) converge faster.\n",
        "* Algorithms relying on **distance metrics** (like KNN, SVM) perform better.\n",
        "* Avoids bias toward features with larger scales.\n",
        "\n",
        "---\n",
        "\n",
        "### Example in Python (StandardScaler):\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WdQYnlAX9ZJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "8eytMMeE9eYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- You can easily perform feature scaling in Python using **scikit-learn’s preprocessing module**. Here are the two most common methods:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Standardization (StandardScaler)**\n",
        "\n",
        "Scales data to have **mean = 0** and **standard deviation = 1**.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your feature matrix (numpy array or DataFrame)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Normalization (MinMaxScaler)**\n",
        "\n",
        "Scales data to a fixed range, usually **0 to 1**.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### How it works:\n",
        "\n",
        "* `.fit_transform()` computes scaling parameters (like mean, std, min, max) from your data and applies the scaling.\n",
        "* For new data, use `.transform()` only, to apply the same scaling learned.\n",
        "\n",
        "---\n",
        "\n",
        "### Example with sample data:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "X = np.array([[10, 200],\n",
        "              [15, 300],\n",
        "              [20, 400]])\n",
        "\n",
        "# Standardization\n",
        "scaler_std = StandardScaler()\n",
        "X_std = scaler_std.fit_transform(X)\n",
        "\n",
        "# Normalization\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_norm = scaler_minmax.fit_transform(X)\n",
        "\n",
        "print(\"Standardized data:\\n\", X_std)\n",
        "print(\"Normalized data:\\n\", X_norm)\n",
        "```\n"
      ],
      "metadata": {
        "id": "G7zTi4MN9qZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "ZmYLXpB_9yuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ansa;- **`sklearn.preprocessing`** is a module in **scikit-learn** that provides tools to **prepare and transform your data** before training machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "### What does it offer?\n",
        "\n",
        "* **Scaling:** Standardize or normalize features (e.g., `StandardScaler`, `MinMaxScaler`)\n",
        "* **Encoding:** Convert categorical data to numeric (e.g., `OneHotEncoder`, `LabelEncoder`)\n",
        "* **Binarizing:** Convert data to binary values (e.g., `Binarizer`)\n",
        "* **Generating polynomial features:** (e.g., `PolynomialFeatures`)\n",
        "* **Imputation & normalization utilities**\n",
        "\n",
        "---\n",
        "\n",
        "### Why use it?\n",
        "\n",
        "* Many machine learning algorithms require data in a certain format or scale.\n",
        "* Preprocessing improves model performance and training speed.\n",
        "\n",
        "---\n",
        "\n",
        "### Example usage:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HfpBXOYf92ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "KfAMgv9q97ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:- You can split data for training and testing in Python using **scikit-learn’s** `train_test_split` function.\n",
        "\n",
        "Here’s how:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = features, y = target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* `test_size=0.2` means 20% of data goes to testing, 80% to training.\n",
        "* `random_state=42` makes the split reproducible.\n",
        "\n",
        "After this, you train your model on `X_train, y_train` and evaluate on `X_test, y_test`.\n",
        "\n"
      ],
      "metadata": {
        "id": "r72CTn6b9-xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?"
      ],
      "metadata": {
        "id": "kujr78ty-Gay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:-\n",
        "\n",
        "**Data encoding** is the process of converting **categorical variables** (non-numeric data) into a **numeric format** so that machine learning algorithms can work with them.\n",
        "\n",
        "---\n",
        "\n",
        "### Why encode data?\n",
        "\n",
        "* Most ML algorithms only understand numbers, not categories like \"Red,\" \"Blue,\" or \"Male,\" \"Female.\"\n",
        "* Encoding transforms categories into numbers without losing their meaning.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Encoding Techniques:\n",
        "\n",
        "| Technique            | Description                                 | Example                          |\n",
        "| -------------------- | ------------------------------------------- | -------------------------------- |\n",
        "| **Label Encoding**   | Assigns each category a unique integer      | Red → 0, Blue → 1, Green → 2     |\n",
        "| **One-Hot Encoding** | Creates binary columns for each category    | Red → \\[1,0,0], Blue → \\[0,1,0]  |\n",
        "| **Ordinal Encoding** | Similar to label encoding but assumes order | Small → 0, Medium → 1, Large → 2 |\n",
        "\n",
        "---\n",
        "\n",
        "### Example: One-Hot Encoding with pandas\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "\n",
        "encoded = pd.get_dummies(df['Color'])\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "   Blue  Green  Red\n",
        "0     0      0    1\n",
        "1     1      0    0\n",
        "2     0      1    0\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-q_q1oct-KKK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6tXI9QV-Shh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}